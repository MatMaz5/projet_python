{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9d503b07-348b-4042-b568-4528d5965ff4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6513e49-b1cc-4604-b563-cf8b1809fc6a",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td width=15%><img src=\"../../img/UGA.png\"></img></td>\n",
    "<td><center><h1>Project n°3</h1></center></td>\n",
    "<td width=15%><a href=\"https://team.inria.fr/tripop/team-members/\" style=\"font-size: 16px; font-weight: bold\">Florian Vincent</a> </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20647865-8316-4ee0-a021-ed961be14b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6901f496-9a78-4b60-a931-303320d3c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca73c79-e5cb-49cf-bafa-6e2496d4e1e5",
   "metadata": {},
   "source": [
    "# Learning text classification\n",
    "\n",
    "This project is heavily inspired from [Jigsaw's *Toxic Comments Classification* challenge](https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/overview) on kaggle.\n",
    "To avoid copy-pastings of foreign code, it will guide you towards specific tools to test and use.\n",
    "\n",
    "## Overview of the project\n",
    "\n",
    "Take a look at the *zip*ed csv data files by unzipping them (`for name in $(ls *.zip); do unzip $name; done;`).\n",
    "\n",
    "Every comment in the train set is classified with a label in `{\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity hate\"}`.\n",
    "You will need to train multiple kind of models to identify those comments, and you will test them against the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf0d2de-b876-4cc1-a4e0-56f815616a9b",
   "metadata": {},
   "source": [
    "# ***Début du projet***\n",
    "\n",
    "Dans cette section, nous chargeons les fichiers de données CSV contenant les commentaires et leurs étiquettes associées. Ces données sont divisées en trois parties :\n",
    "- **Entraînement** : Les commentaires avec leurs labels pour entraîner le modèle.\n",
    "- **Test** : Les commentaires sans labels pour l'évaluation.\n",
    "- **Labels de test** : Les étiquettes correspondantes aux données test, utilisées pour évaluer la performance du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281289d0-29d7-41a9-999e-9b168e200de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chargement des données\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "test_labels = pd.read_csv(\"test_labels.csv\")\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d264c746-9ca0-4cac-a096-6d31e66fd241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               0\n",
       "comment_text     0\n",
       "toxic            0\n",
       "severe_toxic     0\n",
       "obscene          0\n",
       "threat           0\n",
       "insult           0\n",
       "identity_hate    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recherche des valeurs manquantes\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de9d034-79f0-46d6-9f77-a12fd665deee",
   "metadata": {},
   "source": [
    "# ***Nettoyage des données***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d93417bb-d06a-402a-9b93-4354b312666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression de la ponctuation et des sauts de lignes\n",
    "train[\"comment_clean\"] = train[\"comment_text\"].apply(lambda x : re.sub(\"[^a-zA-Z]\", ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d833cc6-8661-4f08-973f-89441162e33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion en minuscule\n",
    "train[\"comment_clean\"] = train[\"comment_clean\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9308a5a7-41a9-4112-927e-c6aae0bab182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>comment_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>d aww  he matches this background colour i m s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>hey man  i m really not trying to edit war  it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>more i can t make any real suggestions on im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>you  sir  are my hero  any chance you remember...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>congratulations from me as well  use the to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...   \n",
       "1  D'aww! He matches this background colour I'm s...   \n",
       "2  Hey man, I'm really not trying to edit war. It...   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4  You, sir, are my hero. Any chance you remember...   \n",
       "5  \"\\n\\nCongratulations from me as well, use the ...   \n",
       "\n",
       "                                       comment_clean  \n",
       "0  explanation why the edits made under my userna...  \n",
       "1  d aww  he matches this background colour i m s...  \n",
       "2  hey man  i m really not trying to edit war  it...  \n",
       "3    more i can t make any real suggestions on im...  \n",
       "4  you  sir  are my hero  any chance you remember...  \n",
       "5     congratulations from me as well  use the to...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualisation des données d'entraînement après les modifications précédentes\n",
    "train[[\"comment_text\", \"comment_clean\"]].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0da52ba-9ab5-4a35-9f7b-1505b25d2eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation (séparation mot à mot)\n",
    "train[\"comment_clean\"] = train[\"comment_clean\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96de6cb5-bc4b-48a0-bffd-528facef801f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>comment_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>[d, aww, he, matches, this, background, colour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>[hey, man, i, m, really, not, trying, to, edit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>[more, i, can, t, make, any, real, suggestions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>[you, sir, are, my, hero, any, chance, you, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>[congratulations, from, me, as, well, use, the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...   \n",
       "1  D'aww! He matches this background colour I'm s...   \n",
       "2  Hey man, I'm really not trying to edit war. It...   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4  You, sir, are my hero. Any chance you remember...   \n",
       "5  \"\\n\\nCongratulations from me as well, use the ...   \n",
       "\n",
       "                                       comment_clean  \n",
       "0  [explanation, why, the, edits, made, under, my...  \n",
       "1  [d, aww, he, matches, this, background, colour...  \n",
       "2  [hey, man, i, m, really, not, trying, to, edit...  \n",
       "3  [more, i, can, t, make, any, real, suggestions...  \n",
       "4  [you, sir, are, my, hero, any, chance, you, re...  \n",
       "5  [congratulations, from, me, as, well, use, the...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualisation des données d'entraînement après les modifications précédentes\n",
    "train[[\"comment_text\", \"comment_clean\"]].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16bb43f9-6205-4109-9a36-19ab284523eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des stopwords (mots de \"liaisons\" inutiles)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "train[\"comment_clean\"] = train[\"comment_clean\"].apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac38bb-4147-43df-91dd-7c5b4b59ce84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>comment_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>[aww, matches, background, colour, seemingly, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>[hey, man, really, trying, edit, war, guy, con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>[make, real, suggestions, improvement, wondere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>[sir, hero, chance, remember, page]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>[congratulations, well, use, tools, well, talk]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...   \n",
       "1  D'aww! He matches this background colour I'm s...   \n",
       "2  Hey man, I'm really not trying to edit war. It...   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4  You, sir, are my hero. Any chance you remember...   \n",
       "5  \"\\n\\nCongratulations from me as well, use the ...   \n",
       "\n",
       "                                       comment_clean  \n",
       "0  [explanation, edits, made, username, hardcore,...  \n",
       "1  [aww, matches, background, colour, seemingly, ...  \n",
       "2  [hey, man, really, trying, edit, war, guy, con...  \n",
       "3  [make, real, suggestions, improvement, wondere...  \n",
       "4                [sir, hero, chance, remember, page]  \n",
       "5    [congratulations, well, use, tools, well, talk]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualisation des données d'entraînement après les modifications précédentes\n",
    "train[[\"comment_text\", \"comment_clean\"]].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55e51ce8-0544-485f-b9be-cc3ddefc69df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatisation (suppression des pluriels, conjugaisons, etc.)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "train[\"comment_clean\"] = train[\"comment_clean\"].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f73eb39-9765-4a5a-990b-b5bde62a2f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>comment_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>[aww, match, background, colour, seemingly, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>[hey, man, really, trying, edit, war, guy, con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>[make, real, suggestion, improvement, wondered...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>[sir, hero, chance, remember, page]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>[congratulation, well, use, tool, well, talk]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...   \n",
       "1  D'aww! He matches this background colour I'm s...   \n",
       "2  Hey man, I'm really not trying to edit war. It...   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4  You, sir, are my hero. Any chance you remember...   \n",
       "5  \"\\n\\nCongratulations from me as well, use the ...   \n",
       "\n",
       "                                       comment_clean  \n",
       "0  [explanation, edits, made, username, hardcore,...  \n",
       "1  [aww, match, background, colour, seemingly, st...  \n",
       "2  [hey, man, really, trying, edit, war, guy, con...  \n",
       "3  [make, real, suggestion, improvement, wondered...  \n",
       "4                [sir, hero, chance, remember, page]  \n",
       "5      [congratulation, well, use, tool, well, talk]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualisation des données d'entraînement après les modifications précédentes\n",
    "train[[\"comment_text\", \"comment_clean\"]].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368e84f-7803-42a0-80c4-805f77a79e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconversion des listes en chaînes de charactères\n",
    "train[\"comment_clean\"] = train[\"comment_clean\"].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4481bf31-345d-4104-a73d-42291cb40c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>comment_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>aww match background colour seemingly stuck th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>make real suggestion improvement wondered sect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>congratulation well use tool well talk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...   \n",
       "1  D'aww! He matches this background colour I'm s...   \n",
       "2  Hey man, I'm really not trying to edit war. It...   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4  You, sir, are my hero. Any chance you remember...   \n",
       "5  \"\\n\\nCongratulations from me as well, use the ...   \n",
       "\n",
       "                                       comment_clean  \n",
       "0  explanation edits made username hardcore metal...  \n",
       "1  aww match background colour seemingly stuck th...  \n",
       "2  hey man really trying edit war guy constantly ...  \n",
       "3  make real suggestion improvement wondered sect...  \n",
       "4                      sir hero chance remember page  \n",
       "5             congratulation well use tool well talk  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualisation des données d'entraînement après les modifications précédentes\n",
    "train[[\"comment_text\", \"comment_clean\"]].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199bb327-c1c8-4347-83fd-9e53287575a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regroupement de toutes les modifications précédentes dans une fonction\n",
    "# Afin de pouvoir les effectuer plus rapidement sur tous les fichiers (train et test)\n",
    "def nettoyage(df) :\n",
    "    # Suppression de la ponctuation et des sauts de lignes\n",
    "    df[\"comment_clean\"] = df[\"comment_text\"].apply(lambda x : re.sub(\"[^a-zA-Z]\", ' ', x))\n",
    "\n",
    "    # Conversion en minuscule\n",
    "    df[\"comment_clean\"] = df[\"comment_clean\"].str.lower()\n",
    "\n",
    "    # Tokenisation\n",
    "    df[\"comment_clean\"] = df[\"comment_clean\"].apply(word_tokenize)\n",
    "\n",
    "    # Suppression des stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    df[\"comment_clean\"] = df[\"comment_clean\"].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "    # Lemmatisation\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df[\"comment_clean\"] = df[\"comment_clean\"].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "    # Reconversion des listes en chaînes de charactères\n",
    "    df[\"comment_clean\"] = df[\"comment_clean\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Attention : la colonne de texte à traiter doit impérativement s'appeler \"comment_text\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bad6c4-2af8-4574-85d5-8011a04cef5c",
   "metadata": {},
   "source": [
    "# ***Vectorisation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "400fe107-4089-43a1-8c60-c7915a9f1cd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[0;32m----> 3\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomment_clean\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Fit and transform the documents into a word count matrix\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "documents = train[\"comment_clean\"]\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "# Adaptation et transformation des documents en une matrice comptant le nombre d'occurences des mots\n",
    "X = vectorizer.fit_transform(documents)\n",
    "print(\"Shape of sparse matrix:\", X.shape)  # Dimensions : nombre de documents × nombre de mots uniques\n",
    "print(\"Vocabulary:\\n\", vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0887aa-77ff-4f01-8f1c-0749d30730da",
   "metadata": {},
   "source": [
    "La méthode TF-IDF attribue un poids à chaque mot en fonction de sa fréquence dans un document et de son importance. Cela permet de réduire l'impact des mots très fréquents comme \"le\", \"est\",..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54080c85-6342-4465-805f-c6baf8e22232",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Charger les documents\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomment_clean\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Initialiser TfidfVectorizer avec réduction de vocabulaire\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#min_df=2 signifie que seuls les mots apparaissant dans au moins 2 documents seront inclus.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#max_df=0.8 signifie que les mots présents dans plus de 80% des documents seront ignorés.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# Chargement des documents\n",
    "documents = train[\"comment_clean\"]\n",
    "\n",
    "# Initialisisation TfidfVectorizer avec réduction de vocabulaire\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, min_df=2, max_df=0.9)\n",
    "    # min_df = 2 signifie que seuls les mots apparaissant dans au moins 2 documents seront inclus.\n",
    "    # max_df = 0.9 signifie que les mots présents dans plus de 90% des documents seront ignorés.\n",
    "\n",
    "# Transformation en matrice TF-IDF\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Résumé de la matrice\n",
    "print(\"Shape of sparse matrix (TF-IDF):\", X_tfidf.shape)\n",
    "print(\"Feature names (vocabulary example):\", tfidf_vectorizer.get_feature_names_out()[:10])\n",
    "\n",
    "# Choix du 3ème commentaire pour inspection\n",
    "first_comment_tfidf = X_tfidf[2].toarray()[0]\n",
    "\n",
    "# Obtention du vocabulaire (features)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Création d'une liste des mots et de leurs poids TF-IDF (non nuls uniquement)\n",
    "non_zero_indices = first_comment_tfidf.nonzero()[0]\n",
    "words_weights = [(feature_names[i], first_comment_tfidf[i]) for i in non_zero_indices]\n",
    "\n",
    "# Trie des mots par ordre décroissant de poids\n",
    "words_weights_sorted = sorted(words_weights, key=lambda x: x[1], reverse=True)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, min_df=2, max_df=0.9)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"\\nWords and their TF-IDF weights for the first comment:\")\n",
    "for word, weight in words_weights_sorted:\n",
    "    print(f\"{word}: {weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a1c7fd-0bf4-4f69-b1fe-d42535c428db",
   "metadata": {},
   "source": [
    "# Réduction de dimensions\n",
    "**Find a suitable dimension reduction technique to study the structure of the data. Display your findings with visual means (you can use `seaborn`).**\n",
    "L'ACP classique, telle qu'implémentée dans sklearn.decomposition.PCA, nécessite que la matrice de données soit dense. Cela implique que, pour les matrices sparse (creuses, contenant majoritairement des zéros), une conversion en matrice dense est nécessaire avant de pouvoir effectuer l'ACP, ce qui peut entraîner une consommation excessive de mémoire.\n",
    "\n",
    "En revanche, TruncatedSVD est spécifiquement conçu pour traiter directement les matrices sparse, telles que celles générées par les représentations TF-IDF en NLP. Cela permet d'économiser une quantité significative d'espace mémoire en évitant toute conversion inutile.\n",
    "\n",
    "Par ailleurs, l'ACP classique repose sur une décomposition complète des matrices, via une méthode connue sous le nom de décomposition en valeurs propres (eigendecomposition). Cette approche est particulièrement coûteuse en temps pour des matrices de grande taille et consomme une quantité importante de mémoire.\n",
    "\n",
    "À l'inverse, TruncatedSVD utilise une technique appelée décomposition SVD tronquée, qui calcule uniquement les premiers n composants principaux sans générer l'intégralité de la matrice de covariance. Cela rend TruncatedSVD non seulement plus rapide, mais également bien plus efficace pour manipuler des matrices de grande dimension, tout en réduisant le coût computationnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ed387a-a14b-49c8-b069-3ef33d7a539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réduction de dimensions avec TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components = 2)  # Réduction à 2 dimensions pour visualisation\n",
    "X_svd = svd.fit_transform(X_tfidf)  # Pas besoin de convertir en dense\n",
    "\n",
    "# Chargement des étiquettes \n",
    "labels = train[\"toxic\"]\n",
    "\n",
    "# Création d'un DataFrame pour combiner les composantes SVD et les étiquettes\n",
    "svd_df = pd.DataFrame(data = X_svd, columns = [\"SVD1\", \"SVD2\"])\n",
    "svd_df[\"Label\"] = labels\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    x=\"SVD1\", y=\"SVD2\", hue=\"Label\", data=svd_df, palette=\"viridis\", alpha=0.6\n",
    ")\n",
    "plt.title(\"TruncatedSVD Visualization of Text Data\")\n",
    "plt.xlabel(\"SVD Component 1\")\n",
    "plt.ylabel(\"SVD Component 2\")\n",
    "plt.legend(title=\"Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2779fc75-9dea-4150-85ab-7670d5bc1ec8",
   "metadata": {},
   "source": [
    "La majorité des points (documents) se situent dans une région dense autour des coordonnées proches de (0.2, 0.2).\n",
    "Les données sont bien projetées sur deux dimensions, ce qui permet une interprétation visuelle.\n",
    "Les points en bleu (label 0) sont largement dominants dans les données, ce qui indique un déséquilibre des classes (beaucoup plus de textes non toxiques que toxiques)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02af5da3-b96e-443d-aebd-d2f5b5a41c9c",
   "metadata": {},
   "source": [
    "Il est donc nécessaire de vérifier le ratio entre les classes 0 et 1 pour confirmer le déséquilibre. \\\n",
    "Ce déséquilibre pour toxic n'est pas dû à SVD, mais aux proportions intrinsèques des données. SVD ne favorise pas une classe ou une autre ; il se concentre uniquement sur la variance des données. \\\n",
    "SVD gère efficacement les matrices creuses de TF-IDF et conserve des tendances globales dans les données, ce qui est utile pour une analyse exploratoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca17720-5d5d-4739-84ae-e6594c2f2dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[\"toxic\"].value_counts(normalize=True))\n",
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf0a85a-6978-40dc-b92b-3555cb9a5d1f",
   "metadata": {},
   "source": [
    "### Combien de dimensions sont suffisantes ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3958fac-539c-4bec-8ed6-639d882a0375",
   "metadata": {},
   "source": [
    "Une bonne pratique consiste à examiner la variance expliquée cumulée pour choisir un nombre optimal de dimensions. En analysant la courbe cumulative de la variance expliquée (avec cumulative_variance), nous constatons que les 300 premières dimensions couvrent généralement 35% de la variance totale dans des tâches de ce projet NLP. Cela signifie que la majorité des informations pertinentes des données d'origine est maintenue. En choisissant 300 dimensions, couvrant déjà la majorité de la variance expliquée, des valeurs plus élevées risquent d'augmenter le coût computationnel sans apporter de gain significatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93175703-bc67-4adf-a487-6f048fd1746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustement SVD avec plus de composantes pour analyser la variance expliquée\n",
    "svd = TruncatedSVD(n_components=1000, random_state=42)\n",
    "svd.fit(X_tfidf)\n",
    "\n",
    "# Variance expliquée cumulative\n",
    "cumulative_variance = svd.explained_variance_ratio_.cumsum()\n",
    "\n",
    "# Visualisation\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance)\n",
    "plt.axhline(y=0.35, color='r', linestyle='--')  # Ligne pour 35% de variance expliquée\n",
    "plt.xlabel(\"Nombre de dimensions\")\n",
    "plt.ylabel(\"Variance expliquée cumulée\")\n",
    "plt.title(\"Analyse de la Variance Expliquée par TruncatedSVD\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987fe89f-041d-40f1-890e-6714258c6169",
   "metadata": {},
   "source": [
    "D'après la figure ci dessous, nous pouvons observer que les premières composantes expliquent la majeure partie de la variance. La variance expliquée diminue rapidement après les premières composantes (effet d'\"écrasement\"). Cela indique qu'on peut probablement réduire de manière significative le nombre de dimensions sans perdre beaucoup d'information. Ce qui accélérera nos calculs pour les modèles comme la SVM par exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bae5a17-1eef-4842-a7a6-0361c152dff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustement SVD avec plus de composantes pour analyser la variance expliquée\n",
    "svd = TruncatedSVD(n_components=1000, random_state=42)\n",
    "svd.fit(X_tfidf)\n",
    "\n",
    "# Variance expliquée par chaque composante\n",
    "explained_variance = svd.explained_variance_ratio_\n",
    "\n",
    "# Visualisation de l'histogramme avec les 300 premières dimensions en rouge\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['red' if i < 300 else 'blue' for i in range(len(explained_variance))]\n",
    "plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, color=colors, edgecolor='black')\n",
    "plt.axvline(x=300, color='green', linestyle='--', label=\"300 Dimensions (Ligne de coupe)\")  # Ligne pour 300 dimensions\n",
    "plt.xlabel(\"Numéro de la composante\")\n",
    "plt.ylabel(\"Variance expliquée\")\n",
    "plt.title(\"Histogramme de la Variance Expliquée par TruncatedSVD\")\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94b47bd-60e8-409b-aac6-9fbbfcf0d7d3",
   "metadata": {},
   "source": [
    "# ***Classifications***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7346f8-5266-4a25-8aae-41bf42227f55",
   "metadata": {},
   "source": [
    "### ***Préparation des données communes***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba356e15-9be9-4556-8666-f92db4d1ce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des fichiers CSV\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "test_labels = pd.read_csv(\"test_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ded863f-7838-4cb7-8d4a-ec0dd9573249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage des données\n",
    "train = nettoyage(train)\n",
    "test = nettoyage(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177eb0b8-3d6f-41de-bdc5-c3af4d7e0bf6",
   "metadata": {},
   "source": [
    "Les étiquettes des données de test contiennent des valeurs `-1`, qui indiquent des commentaires sans étiquettes valides.  \n",
    "Nous les supprimons donc des jeux de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8639179d-7e00-4d57-9600-4e5dec496adc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m[test_labels[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoxic\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      2\u001b[0m test_labels \u001b[38;5;241m=\u001b[39m test_labels[test_labels[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoxic\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "test = test[test_labels[\"toxic\"] != -1]\n",
    "test_labels = test_labels[test_labels[\"toxic\"] != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f794747-5cd0-44d7-9edd-2f9435a2d2b5",
   "metadata": {},
   "source": [
    "### ***Régression Logistique***\n",
    "\n",
    "La régression logistique est un modèle simple mais efficace pour une classification binaire.  \n",
    "Nous entraînons un modèle pour prédire si un commentaire est \"toxique\" (`1`) ou \"non-toxique\" (`0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132dd8f4-9711-435b-beba-8cfbff3c9c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## with the use of tf-idf the words thats are more accurate are less important so we wont have the redondant words when it comes to counting from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# The TfidfVectorizer: tokenazation  splits the text into smaller units called tokens (usually words)\n",
    "# then vectorizer calculates the TF-IDF (Term Frequency-Inverse Document Frequency) value for each term in the corpus.\n",
    "# which is the number of a token on the total number of all tokens  for term freq \n",
    "# Inverse Document Frequency or idf : is simply the log of the number of  documents or sentences thats containes the token (word)\n",
    "\n",
    "# Pipeline avec l'étape de TF-IDF\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000))  # Étape TF-IDF\n",
    "])\n",
    "\n",
    "# Chargement du corpus\n",
    "corpus1 = train[\"comment_clean\"]\n",
    "\n",
    "# Application du pipeline\n",
    "X_tfidf = pipeline.fit_transform(corpus1)\n",
    "\n",
    "# Affichage des dimensions et des vecteurs train \n",
    "print(\"Shape de X_train :\", X_tfidf.shape)  # nombre de documents x nombre de tokens (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a19f350-3b41-44d1-b0b4-3cc4a1f86969",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.merge(test_labels, on = \"id\")\n",
    "# Convertir les colonnes à partir de la 3ème colonne en numériques\n",
    "# test.iloc[:, 3:] = test.iloc[:, 3:].apply(pd.to_numeric, errors='coerce')\n",
    "# Filtrage des lignes où la somme des colonnes à partir de la 3ème est >= 0\n",
    "test = test[test.iloc[:, 3:].sum(axis=1) >= 0]\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fb3bc2-0b3d-4cce-bc0f-4f14e4b8f8b7",
   "metadata": {},
   "source": [
    "**Paramètres choisis :**\n",
    "- `solver='liblinear'` : Adapté aux  problèmes de classification binaire.\n",
    "- `C=1.0` : Paramètre de régularisation (valeur par défaut).\n",
    "- `class_weight='balanced'` : Pour compenser le déséquilibre des classes.\n",
    "\n",
    "Nous évaluerons le modèle avec un rapport de classification et une matrice de confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dec537-435a-4b39-8667-5810b492f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "## logistic classifier \n",
    "# les commentaires toxiques.\n",
    "model = LogisticRegression(solver='liblinear', C=0.1,class_weight='balanced', random_state=0)\n",
    "\n",
    "# X : Les commentaires transformés en vecteurs numériques avec TF-IDF\n",
    "# X_tfidf est la matrice résultante de la transformation des commentaires nettoyés en une représentation numérique \n",
    "# (chaque ligne est un commentaire, et chaque colonne représente un mot, avec les valeurs TF-IDF associées).\n",
    "X_train = X_tfidf\n",
    "# Y : Le label (binaire pour toxicité)\n",
    "# Ici, nous utilisons la colonne 'toxic' de 'train' comme étiquette cible.\n",
    "# Cette colonne contient des valeurs 0 ou 1, représentant si le commentaire est toxique ou non.\n",
    "y_train = train[\"toxic\"]\n",
    "# Entraînement du modèle avec les données X_train (les vecteurs des commentaires)\n",
    "# et y_train (les étiquettes correspondantes de toxicité).\n",
    "model.fit(X_train, y_train)\n",
    "# Après l'entraînement du modèle, on peut maintenant faire des prédictions sur de nouvelles données.\n",
    "# Affiche les classes que le modèle peut prédire.\n",
    "# Cela montre les étiquettes de classification possibles (dans ce cas, 'toxic' ou 'non-toxic' pour un modèle binaire).\n",
    "# présenter par la liste 0,1\n",
    "print(model.classes_)\n",
    "# Affiche l'ordonnée à l'origine du modèle (le biais).\n",
    "print(model.intercept_)\n",
    "# Affiche les coefficients du modèle pour chaque caractéristique (mot dans ce cas).\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04380a07",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "# Sauvegarde du modèle et du vectoriseur avec pickle\n",
    "with open('logistic_regression_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "\n",
    "with open('vectorizer.pkl', 'wb') as vectorizer_file:\n",
    "    pickle.dump(tfidf_vectorizer, vectorizer_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c519d61c-7704-4941-9a4c-40b1d2eff38b",
   "metadata": {},
   "source": [
    "#### Le modèle régression logistique avec ACP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5970be97",
   "metadata": {},
   "source": [
    "Les résultats sont similaires avec ou sans ACP, dans ce cas présent. Nous avons donc sélectionné celui sans ACP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23064463-efd3-4b9f-90b6-89562eaf954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # avec pca \n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# # Étape 1 : Réduction de la dimensionnalité avec PCA\n",
    "# pca = PCA(n_components=300, random_state=0)\n",
    "# X_train_reduced = pca.fit_transform(X_train)\n",
    "\n",
    "# # Étape 2 : Régression logistique avec les données réduites\n",
    "# model2 = LogisticRegression(solver='liblinear', C=0.1, class_weight='balanced', random_state=0)\n",
    "\n",
    "# # Entraînement du modèle\n",
    "# model2.fit(X_train_reduced, y_train)\n",
    "\n",
    "# # Étape 3 : Résultats du modèle\n",
    "# print(\"Classes prédites :\", model2.classes_)\n",
    "# print(\"Ordonnée à l'origine (biais) :\", model2.intercept_)\n",
    "# print(\"Coefficients du modèle :\", model2.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e3465c-6e25-434e-b553-51d1aea9600d",
   "metadata": {},
   "source": [
    "#### Évaluation sur les données de test\n",
    "\n",
    "Nous utilisons le modèle pour prédire les étiquettes des données de test et évaluons sa performance avec :\n",
    "1. Un **rapport de classification** (précision, rappel, F1-score).\n",
    "2. Une **matrice de confusion** pour visualiser les résultats.\n",
    "\n",
    "Cela nous donne des insights sur la capacité du modèle à généraliser sur les autre labels ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90bca8e-950a-4183-9232-c9891a41cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions sur les données de test\n",
    "# Transformation des données de test avec le même vectoriseur (sans fit)\n",
    "X_test_tfidf_t = pipeline.transform(test[\"comment_clean\"])  \n",
    "y_test = test[\"toxic\"]  # Remplace par les vraies étiquettes de test\n",
    "\n",
    "# Prédictions\n",
    "y_pred = model.predict(X_test_tfidf_t)\n",
    "\n",
    "# Affichage du rapport de classification\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Affichage de la matrice de confusion\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.imshow(cm)\n",
    "ax.grid(False)\n",
    "ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0', 'Predicted 1'))\n",
    "ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0', 'Actual 1'))\n",
    "ax.set_ylim(1.5, -0.5)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9dd7b9-4434-4006-9257-c2ae6b14bb8e",
   "metadata": {},
   "source": [
    "#### Classification multi-étiquettes avec Régression Logistique\n",
    "\n",
    "Pour traiter plusieurs étiquettes (`toxic`, `severe_toxic`, etc.), nous utilisons un classifieur multi-sortie (`MultiOutputClassifier`).  \n",
    "Chaque étiquette est prédite individuellement avec un modèle de régression logistique.\n",
    "Voici les choix des paramètres pour ce modèle :\n",
    "\n",
    "1. **`solver='liblinear'`** :  \n",
    "   Ce solveur est bien adapté aux problèmes de classification binaire. Chaque étiquette étant une classification binaire dans notre cas, ce solveur est optimal.\n",
    "\n",
    "2. **`C=1.0`** :  \n",
    "   Ce paramètre contrôle la régularisation. Une valeur par défaut de `1.0` équilibre la régularisation et l’ajustement du modèle. Cela permet de prévenir le surapprentissage sans sous-apprentissage.\n",
    "\n",
    "3. **`class_weight='balanced'`** :  \n",
    "   Les classes dans les données sont déséquilibrées (ex. peu de commentaires `toxic` par rapport aux `non-toxic`). Ce paramètre ajuste automatiquement les poids des classes en fonction de leur fréquence dans les données d'entraînement, ce qui améliore les performances pour les classes rares.\n",
    "\n",
    "4. **`random_state=0`** :  \n",
    "   Permet de fixer une graine pour la reproductibilité des résultats.\n",
    "   \n",
    "Les résultats pour chaque étiquette sont évalués à l'aide des métriques suivantes :\n",
    "- Précision : proportion des prédictions positives (1) correctes parmi toutes les prédictions positives. Elle évalue combien des prédictions positives sont réellement correctes.\n",
    "- Rappel : proportion des exemples positifs correctement identifiés parmi tous les exemples positifs. Il mesure la capacité du modèle à trouver tous les cas positifs.\n",
    "- F1-score : mesure qui combine la précision et le rappel en une seule valeur. Elle donne une moyenne harmonique des deux et est particulièrement utile lorsque les classes sont déséquilibrées.\n",
    "- Exactitude (accuracy) : proportion des prédictions correctes, positives ou négatives, par rapport à l'ensemble des prédictions totales. Elle évalue la performance globale du modèle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a6b1f7-ad74-4f09-8aaf-ae3c62b84ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des étiquettes\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# Données cibles multi-étiquettes (binaire pour chaque étiquette)\n",
    "y_train_m = train[labels]\n",
    "X_train_m = X_tfidf\n",
    "# Modèle multi-étiquette\n",
    "multi_target_model = MultiOutputClassifier(LogisticRegression(solver='liblinear', C=1.0,class_weight='balanced', random_state=0))\n",
    "\n",
    "# Entraînement du modèle\n",
    "multi_target_model.fit(X_train_m, y_train_m)\n",
    "\n",
    "# Prédictions\n",
    "X_test_tfidf_t = pipeline.transform(test[\"comment_clean\"])\n",
    "y_pred_multi = multi_target_model.predict(X_test_tfidf_t)\n",
    "\n",
    "# Initialisisation d'un DataFrame pour stocker les résultats\n",
    "results = []\n",
    "\n",
    "# Évaluation de chaque étiquette\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"Évaluation de l'étiquette: {label}\")\n",
    "    \n",
    "    # Obtention des vraies étiquettes et les prédictions pour chaque étiquette\n",
    "    y_test_label = test[label]\n",
    "    y_pred_label = y_pred_multi[:, i]\n",
    "    \n",
    "    # Calcul du rapport de classification\n",
    "    report = classification_report(\n",
    "        y_test_label, \n",
    "        y_pred_label, \n",
    "        zero_division=0,  # Évite les avertissements\n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    # Calcul de l'accuracy pour chaque étiquette\n",
    "    accuracy = accuracy_score(y_test_label, y_pred_label)\n",
    "    \n",
    "    # Ajout des résultats au tableau\n",
    "    results.append({\n",
    "        \"Label\": label,\n",
    "        \"Precision\": report['weighted avg']['precision'],\n",
    "        \"Recall\": report['weighted avg']['recall'],\n",
    "        \"F1-Score\": report['weighted avg']['f1-score'],\n",
    "        \"Accuracy\": accuracy  # Ajouter l'accuracy à chaque étiquette\n",
    "    })\n",
    "\n",
    "# Affichage des résultats sous forme de tableau\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "print(\"\\n \\n Précision générale : \", accuracy_score(y_test_label,y_pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a074e004-57f4-423f-b352-7275e04b26ac",
   "metadata": {},
   "source": [
    "#### Résultats finaux\n",
    "\n",
    "Le modèle multi-étiquette a permis de prédire les différentes catégories de toxicité.  \n",
    "Les performances (précision, rappel, F1-score) varient selon les étiquettes, mais montrent que la régression logistique est efficace pour ce problème.  \n",
    "Des modèles plus avancés pourraient améliorer ces résultats, notamment pour les classes déséquilibrées.\n",
    "Il existe également un score de permutation permettant d'observer l'importance des variables, ce qui pourrait améliorer le modèle. \n",
    "De plus, cette méthode donne des résultats probabilistes pour chaque individu d'appartenir à une classe, ce qui permet d'effectuer ensuite des tests et donc d'observer également les p-valeurs. \n",
    "Cette méthode peut apporter également d'autres indicateurs tels que les odds-ratio, qui apportent d'autres informations et d'autres interprétations sur les résultats du modèle.\n",
    "\n",
    "#### Les classes predites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "518f3d94-2823-401c-8326-c73e607ecc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maintenant, pour chaque commentaire, on print l'étiquette qui a la plus haute probabilité .\n",
    "# for i, pred in enumerate(y_pred_multi):\n",
    "    # pred_class = labels[pred.argmax()]                   # Trouver l'indice de la classe avec la plus haute probabilité\n",
    "    # print(f\"Commentaire {i+1} : {pred_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17830d6a-ccf5-4ccc-af00-f1515517265c",
   "metadata": {},
   "source": [
    "### ***SVM***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d2fa00-23bd-44d4-8f96-09179b856cd4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Le sous-échantillonnage a été choisi pour travailler avec un volume de données gérable tout en préservant la diversité, ce qui améliore la vitesse d'exécution. La combinaison TF-IDF et SVD a été utilisée pour convertir efficacement les textes en vecteurs numériques et réduire leur dimensionnalité, préparant ainsi les données pour un traitement optimal par le SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268441ad-fc55-4449-b2a7-958c9ba7a625",
   "metadata": {},
   "source": [
    "#### Tester que sur 15 000 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44968ba4-8a1e-4968-b394-4dc188127da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réduction du nombre d'observations \n",
    "train = train.sample(n=15000, random_state=42)\n",
    "test = test.sample(n=10000, random_state=42)\n",
    "\n",
    "# Initialisation du TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_tfidf_train = tfidf_vectorizer.fit_transform(train[\"comment_clean\"])\n",
    "X_tfidf_test = tfidf_vectorizer.transform(test[\"comment_clean\"])\n",
    "\n",
    "# Réduction de dimension avec TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "X_tfidf_train_reduced = svd.fit_transform(X_tfidf_train)\n",
    "X_tfidf_test_reduced = svd.transform(X_tfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89976747-4884-461a-a1a6-718313d88581",
   "metadata": {},
   "source": [
    "Pour tester différents paramètres de C, kernel et gamma et choisir les meilleurs. Nous pouvons utiliser une validation croisée avec un grid search pour explorer plusieurs combinaisons de ces paramètres. Le GridSearchCV de scikit-learn est parfait pour ce genre d'expérimentation.\n",
    "\n",
    "Après une exécution prolongée de 43 minutes, les résultats obtenus pour la catégorie \"toxic\" par exemple sont prometteurs. La recherche des hyperparamètres a révélé une configuration optimale avec un noyau RBF, une valeur de C à 0.1 et un gamma à 0.1. Cette configuration a permis d'atteindre une précision remarquable de 0.9395 sur l'ensemble de validation, et une accuracy de 0.927 sur l'ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f06afa-8f9d-4d0e-beba-7a2266e02488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catégorie cible\n",
    "category = \"toxic\"\n",
    "\n",
    "# Labels pour la catégorie cible\n",
    "y_train_category = train[category]\n",
    "y_test_category = test[category]\n",
    "\n",
    "# Définir la grille de paramètres à tester\n",
    "param_grid = {\n",
    "    'C': [0.1, 1.0, 10.0],  # Valeurs de régularisation\n",
    "    'kernel': ['linear', 'rbf'],  # Noyaux à tester\n",
    "    'gamma': ['scale', 'auto', 0.1, 1.0]  # Paramètre gamma pour le noyau RBF\n",
    "}\n",
    "\n",
    "# Initialisation du SVM\n",
    "svm_clf = SVC(class_weight=\"balanced\", random_state=42)\n",
    "\n",
    "# Initialisation du GridSearchCV avec validation croisée (par exemple, 5 folds)\n",
    "grid_search = GridSearchCV(estimator=svm_clf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Entraîner le modèle sur les données d'entraînement\n",
    "grid_search.fit(X_tfidf_train_reduced, y_train_category)\n",
    "\n",
    "# Afficher les meilleurs paramètres et la meilleure précision\n",
    "print(\"Meilleurs paramètres : \", grid_search.best_params_)\n",
    "print(\"Meilleure précision obtenue : \", grid_search.best_score_)\n",
    "\n",
    "# Utiliser le meilleur modèle pour faire des prédictions sur le test\n",
    "best_svm_clf = grid_search.best_estimator_\n",
    "y_pred_category = best_svm_clf.predict(X_tfidf_test_reduced)\n",
    "\n",
    "# Évaluation des performances\n",
    "print(\"Accuracy sur le test :\", accuracy_score(y_test_category, y_pred_category))\n",
    "print(\"Classification Report sur le test :\\n\", classification_report(y_test_category, y_pred_category))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb1976-5bba-44ad-a4e0-10f3995530d2",
   "metadata": {},
   "source": [
    "#### Sans validation croisée pour la catégorie \"toxic\" :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811652a8-ed7c-408a-b007-12b585c69e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catégorie cible\n",
    "category = \"toxic\"\n",
    "\n",
    "# Labels pour la catégorie cible\n",
    "y_train_category = train[category]\n",
    "y_test_category = test[category]\n",
    "\n",
    "# Initialisation et entraînement du modèle SVM: \n",
    "svm_clf = SVC(kernel=\"rbf\", C=0.1,gamma=0.1, class_weight=\"balanced\",random_state=42)\n",
    "svm_clf.fit(X_tfidf_train_reduced, y_train_category)\n",
    "\n",
    "# Prédiction sur le jeu de test\n",
    "y_pred_category = svm_clf.predict(X_tfidf_test_reduced)\n",
    "\n",
    "# Évaluation des performances\n",
    "print(\"Accuracy:\", accuracy_score(y_test_category, y_pred_category))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_category, y_pred_category))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff68da9-c0b4-4a6c-aed6-1c1deea19df4",
   "metadata": {},
   "source": [
    "#### Avec validation croisée pour la catégorie \"toxic\" :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb699d13-3159-4d61-a59e-55190eda0bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catégorie cible\n",
    "category = \"toxic\"\n",
    "\n",
    "# Labels pour la catégorie cible\n",
    "y_train_category = train[category]\n",
    "y_test_category = test[category]\n",
    "\n",
    "svm_clf = SVC(kernel=\"rbf\", C=0.1, gamma=0.1, class_weight=\"balanced\",random_state=42)\n",
    "\n",
    "# Effectuer la validation croisée  avec 5 plis\n",
    "cv_results = cross_validate(svm_clf, X_tfidf_train_reduced, y_train_category, cv=5, scoring='accuracy', return_train_score=False)\n",
    "\n",
    "# Afficher les résultats de la validation croisée\n",
    "print(\"Précision moyenne de la validation croisée : \", cv_results['test_score'].mean())\n",
    "print(\"Scores de précision pour chaque pli : \", cv_results['test_score'])\n",
    "\n",
    "# Initialisation et entraînement final du modèle SVM avec les données d'entraînement\n",
    "svm_clf.fit(X_tfidf_train_reduced, y_train_category)\n",
    "\n",
    "# Prédiction sur le jeu de test\n",
    "y_pred_category = svm_clf.predict(X_tfidf_test_reduced)\n",
    "\n",
    "# Évaluation des performances\n",
    "print(\"Accuracy sur le test :\", accuracy_score(y_test_category, y_pred_category))\n",
    "print(\"Classification Report sur le test :\\n\", classification_report(y_test_category, y_pred_category))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d588390-eb86-4c39-bd69-6f0f569ea195",
   "metadata": {},
   "source": [
    "En comparant les résultats avec et sans validation croisée, on constate une précision globale (Accuracy) similaire d'environ 0.904, indiquant que la validation croisée n'a pas engendré de surapprentissage ou sous-apprentissage significatif. Cependant, la classification reste fortement déséquilibrée : la classe majoritaire (0) est très bien prédite (précision 0.90, recall 1.00), tandis que la classe minoritaire (1) est mal détectée (précision 0.96, mais recall seulement 0.02, F1-Score 0.04). Bien que la validation croisée n'améliore pas la performance finale, elle démontre la cohérence du modèle à travers les différents plis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a22ab1-c07c-4cc0-bf2b-914a952f422a",
   "metadata": {},
   "source": [
    "#### SVM avec toutes les catégories :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0ea951-cf84-4425-bf3d-c7e6b0925672",
   "metadata": {},
   "source": [
    "Les performances du modèle varient considérablement selon les catégories. Pour \"toxic\" et \"obscene\", les F1-scores de 0.58 et 0.63 respectivement pour la classe 1 sont acceptables, montrant une certaine efficacité dans la détection des classes minoritaires. La catégorie \"insult\" montre également des résultats prometteurs avec un F1-score de 0.56 pour la classe 1, une précision de 0.59 et un rappel de 0.53, ce qui est relativement bon compte tenu du déséquilibre des classes (580 exemples positifs contre 9420 négatifs). Il est important de noter que les accuracy pour toxic, obscene et insult sont respectivement de 0.92, 0.96 et 0.95, ce qui indique une bonne performance globale du modèle pour ces catégories.Cependant, pour \"severe_toxic\", \"threat\", et \"identity_hate\", les performances sont très faibles, avec des précisions et F1-scores extrêmement bas pour les classes 1, malgré parfois un rappel (recall) élevé. Ce problème est principalement dû au déséquilibre important des données, avec moins de 1% d'exemples positifs dans ces catégories, ce qui biaise le modèle SVM vers la classe majoritaire (0) et rend la détection des classes positives très difficile.\n",
    "\n",
    "Enfin, il est à noter que le temps d'exécution pour ce code est d'environ 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31019ad-99f5-4580-970d-99bbef311b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des catégories à tester (par exemple, \"toxic\", \"severe_toxic\", \"obscene\", ...)\n",
    "categories = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "# Résultats pour chaque catégorie\n",
    "for category in categories:\n",
    "    print(f\"=== Catégorie : {category} ===\")\n",
    "    \n",
    "    # Labels pour la catégorie cible\n",
    "    y_train_category = train[category]\n",
    "    y_test_category = test[category]\n",
    "    \n",
    "    # Initialisation et entraînement du modèle SVM\n",
    "    svm_clf = SVC(kernel=\"rbf\", C=0.1, gamma=0.1, class_weight=\"balanced\",random_state=42)\n",
    "    svm_clf.fit(X_tfidf_train_reduced, y_train_category)\n",
    "    \n",
    "    # Prédiction sur le jeu de test\n",
    "    y_pred_category = svm_clf.predict(X_tfidf_test_reduced)\n",
    "    \n",
    "    # Évaluation des performances\n",
    "    print(\"Accuracy:\", accuracy_score(y_test_category, y_pred_category))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test_category, y_pred_category))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb8d911-baf4-48b3-bc36-5db7036ee44e",
   "metadata": {},
   "source": [
    "#### Trouver les bons parametres de SVM pour les catégories severe_toxic, threat et identity_hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03de66ba-9714-497a-88b7-5889e4c93cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catégorie cible\n",
    "category = \"threat\"\n",
    "\n",
    "# Labels pour la catégorie cible\n",
    "y_train_category = train[category]\n",
    "y_test_category = test[category]\n",
    "\n",
    "# Définir la grille de paramètres à tester\n",
    "param_grid = {\n",
    "    'C': [0.1, 1.0, 10.0],  # Valeurs de régularisation\n",
    "    'kernel': ['linear', 'rbf'],  # Noyaux à tester\n",
    "    'gamma': ['scale', 'auto', 0.1, 1.0]  # Paramètre gamma pour le noyau RBF\n",
    "}\n",
    "\n",
    "# Initialisation du SVM\n",
    "svm_clf = SVC(class_weight=\"balanced\", random_state=42)\n",
    "\n",
    "# Initialisation du GridSearchCV avec validation croisée (par exemple, 5 folds)\n",
    "grid_search = GridSearchCV(estimator=svm_clf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Entraîner le modèle sur les données d'entraînement\n",
    "grid_search.fit(X_tfidf_train_reduced, y_train_category)\n",
    "\n",
    "# Afficher les meilleurs paramètres et la meilleure précision\n",
    "print(\"Meilleurs paramètres : \", grid_search.best_params_)\n",
    "print(\"Meilleure précision obtenue : \", grid_search.best_score_)\n",
    "\n",
    "# Utiliser le meilleur modèle pour faire des prédictions sur le test\n",
    "best_svm_clf = grid_search.best_estimator_\n",
    "y_pred_category = best_svm_clf.predict(X_tfidf_test_reduced)\n",
    "\n",
    "# Évaluation des performances\n",
    "print(\"Accuracy sur le test :\", accuracy_score(y_test_category, y_pred_category))\n",
    "print(\"Classification Report sur le test :\\n\", classification_report(y_test_category, y_pred_category))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9256653b-40f5-4af1-8f5a-eac20c871b61",
   "metadata": {},
   "source": [
    "Apres 32 minutes d'execution , on a trouvé que les meilleures parametres pour SVM pour les catégories threat / identity_hate sont: C: 0.1, gamma: auto, kernel: rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26868a9-ca3e-425e-bf40-cb498505000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catégorie cible\n",
    "category = \"identity_hate\"\n",
    "\n",
    "# Labels pour la catégorie cible\n",
    "y_train_category = train[category]\n",
    "y_test_category = test[category]\n",
    "\n",
    "# Définir la grille de paramètres à tester\n",
    "param_grid = {\n",
    "    'C': [0.1, 1.0, 10.0],  # Valeurs de régularisation\n",
    "    'kernel': ['linear', 'rbf'],  # Noyaux à tester\n",
    "    'gamma': ['scale', 'auto', 0.1, 1.0]  # Paramètre gamma pour le noyau RBF\n",
    "}\n",
    "\n",
    "# Initialisation du SVM\n",
    "svm_clf = SVC(class_weight=\"balanced\", random_state=42)\n",
    "\n",
    "# Initialisation du GridSearchCV avec validation croisée (par exemple, 5 folds)\n",
    "grid_search = GridSearchCV(estimator=svm_clf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Entraîner le modèle sur les données d'entraînement\n",
    "grid_search.fit(X_tfidf_train_reduced, y_train_category)\n",
    "\n",
    "# Afficher les meilleurs paramètres et la meilleure précision\n",
    "print(\"Meilleurs paramètres : \", grid_search.best_params_)\n",
    "print(\"Meilleure précision obtenue : \", grid_search.best_score_)\n",
    "\n",
    "# Utiliser le meilleur modèle pour faire des prédictions sur le test\n",
    "best_svm_clf = grid_search.best_estimator_\n",
    "y_pred_category = best_svm_clf.predict(X_tfidf_test_reduced)\n",
    "\n",
    "# Évaluation des performances\n",
    "print(\"Accuracy sur le test :\", accuracy_score(y_test_category, y_pred_category))\n",
    "print(\"Classification Report sur le test :\\n\", classification_report(y_test_category, y_pred_category))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da50567-5a94-4b0a-8bd7-a3bca9c4151b",
   "metadata": {},
   "source": [
    "Pour information : le temps d'execution de ce code est environ 20 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805f5b13-802f-4b1b-b99d-fff62822b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catégorie cible\n",
    "category = \"severe_toxic\"\n",
    "\n",
    "# Labels pour la catégorie cible\n",
    "y_train_category = train[category]\n",
    "y_test_category = test[category]\n",
    "\n",
    "# Définir la grille de paramètres à tester\n",
    "param_grid = {\n",
    "    'C': [0.1, 1.0, 10.0],  # Valeurs de régularisation\n",
    "    'kernel': ['linear', 'rbf'],  # Noyaux à tester\n",
    "    'gamma': ['scale', 'auto', 0.1, 1.0]  # Paramètre gamma pour le noyau RBF\n",
    "}\n",
    "\n",
    "# Initialisation du SVM\n",
    "svm_clf = SVC(class_weight=\"balanced\", random_state=42)\n",
    "\n",
    "# Initialisation du GridSearchCV avec validation croisée (par exemple, 5 folds)\n",
    "grid_search = GridSearchCV(estimator=svm_clf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Entraîner le modèle sur les données d'entraînement\n",
    "grid_search.fit(X_tfidf_train_reduced, y_train_category)\n",
    "\n",
    "# Afficher les meilleurs paramètres et la meilleure précision\n",
    "print(\"Meilleurs paramètres : \", grid_search.best_params_)\n",
    "print(\"Meilleure précision obtenue : \", grid_search.best_score_)\n",
    "\n",
    "# Utiliser le meilleur modèle pour faire des prédictions sur le test\n",
    "best_svm_clf = grid_search.best_estimator_\n",
    "y_pred_category = best_svm_clf.predict(X_tfidf_test_reduced)\n",
    "\n",
    "# Évaluation des performances\n",
    "print(\"Accuracy sur le test :\", accuracy_score(y_test_category, y_pred_category))\n",
    "print(\"Classification Report sur le test :\\n\", classification_report(y_test_category, y_pred_category))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31da956-1a84-4ad9-9b0a-0c93fbc748c1",
   "metadata": {},
   "source": [
    "Les résultats pour la catégorie \"severe_toxic\" montrent une accuracy élevée de 0.9789, avec des performances notables dans la détection des cas positifs. Le F1-score pour la classe positive (1) est de 0.27, tandis que le rappel atteint 0.67, indiquant que le modèle identifie 67% des cas \"severe_toxic\". La précision pour cette classe est de 0.17, ce qui signifie que le modèle fait quelques faux positifs tout en détectant efficacement les vrais positifs. Le macro average F1-score de 0.63 reflète une bonne performance globale. Ces résultats démontrent une sensibilité accrue aux cas minoritaires, bien qu'il reste des opportunités d'amélioration. Pour optimiser davantage les performances du modèle, il serait bénéfique d'explorer des techniques supplémentaires pour traiter le déséquilibre des classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e364ea82-721f-4177-ae7a-4660108c3cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catégorie cible\n",
    "category = \"severe_toxic\"\n",
    "\n",
    "# Labels pour la catégorie cible\n",
    "y_train_category = train[category]\n",
    "y_test_category = test[category]\n",
    "\n",
    "# Initialisation et entraînement du modèle SVM: \n",
    "svm_clf = SVC(kernel=\"rbf\", C=1.0,gamma=\"scale\", class_weight=\"balanced\",random_state=42)\n",
    "svm_clf.fit(X_tfidf_train_reduced, y_train_category)\n",
    "\n",
    "# Prédiction sur le jeu de test\n",
    "y_pred_category = svm_clf.predict(X_tfidf_test_reduced)\n",
    "\n",
    "# Évaluation des performances\n",
    "print(\"Accuracy:\", accuracy_score(y_test_category, y_pred_category))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_category, y_pred_category))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8444ab31-38a1-4c39-8fbe-9d9fd27d372e",
   "metadata": {},
   "source": [
    "Les résultats pour les catégories \"threat\" et \"identity_hate\" révèlent des performances très contrastées : Pour \"threat\", l'accuracy est très élevée (0.9966), mais le modèle échoue complètement à identifier les cas positifs. Les scores de précision, rappel et F1-score pour la classe positive sont tous à 0, indiquant que le modèle classe systématiquement tous les exemples comme négatifs. Cela suggère un problème sévère de déséquilibre des classes, avec seulement 34 exemples positifs sur 10000. Pour \"identity_hate\", les résultats sont inhabituels. L'accuracy est extrêmement basse (0.0125), mais le rappel pour la classe positive est de 1.00, signifiant que le modèle identifie tous les cas positifs. Cependant, la précision très faible (0.01) indique que le modèle classe presque tous les exemples comme positifs, résultant en de nombreux faux positifs. Ce comportement pourrait être dû à une sur-correction du déséquilibre des classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d46c91-d650-4fdf-b059-edb1fa3496d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"threat\",\"identity_hate\"]\n",
    "\n",
    "# Résultats pour chaque catégorie\n",
    "for category in categories:\n",
    "    print(f\"=== Catégorie : {category} ===\")\n",
    "    \n",
    "    # Labels pour la catégorie cible\n",
    "    y_train_category = train[category]\n",
    "    y_test_category = test[category]\n",
    "    \n",
    "    # Initialisation et entraînement du modèle SVM\n",
    "    svm_clf = SVC(kernel=\"rbf\", C=0.1, gamma=\"auto\", class_weight=\"balanced\",random_state=42)\n",
    "    svm_clf.fit(X_tfidf_train_reduced, y_train_category)\n",
    "    \n",
    "    # Prédiction sur le jeu de test\n",
    "    y_pred_category = svm_clf.predict(X_tfidf_test_reduced)\n",
    "    \n",
    "    # Évaluation des performances\n",
    "    print(\"Accuracy:\", accuracy_score(y_test_category, y_pred_category))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test_category, y_pred_category))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50325dd-53a0-452c-b586-31a41046e646",
   "metadata": {},
   "source": [
    "## ***Random Forest***\n",
    "La méthode de Random Forest se base sur **plusieurs** *arbres de décisions* indépendants afin de prédire un modèle plus précis que ceux obtenu par chaque arbre individuellement.\n",
    "Un arbre de décision est un ensemble d'algorithmes permettant de séparer au mieux nos données selon un certains nombre de décisions, représentées par des *branches*.\n",
    "Un arbre est très sensible aux variations des données d'apprentissage.  C'est pour cela qu'une forêt est généralement privilégiée : en combinant les résultats de plusieurs arbres de décisions réalisés sur des données d'apprentissage variables, la forêt aléatoire réduit le risque d'erreurs dû à des changements dans les dites données. C'est donc une méthode robuste.\\\n",
    "De plus, à l'aide de cette méthode, nous pouvons observer l'importance de chaque variable grâce au score de Gini. C'est également une méthode très compréhensible pour un public non formé aux statistiques car elle est très visuelle. \\\n",
    "\n",
    "On peut alors se poser plusieurs question, comme :\n",
    "- Combien de branches utiliser pour chaque arbre ? \n",
    "- Combien d'arbres en tout ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68b3e73-c240-44e9-8e75-94a351bcd452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commentaires et cibles\n",
    "X_train = train[\"comment_clean\"]\n",
    "Y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]]\n",
    "\n",
    "X_test = test[\"comment_clean\"]\n",
    "Y_test = test_labels[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c54409c-8e2c-4333-8e75-f7277ddd8a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectoriser\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d76863-c890-42c4-836e-c02fb96d216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réduction de dimensions\n",
    "X_train_svd = svd.fit_transform(X_train_tfidf)\n",
    "X_test_svd = svd.fit_transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c60b776-9fab-4be7-bf52-0aa1653309e5",
   "metadata": {},
   "source": [
    "Testons d'abord un modèle avec 10 arbres :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505fc55b-ebdd-4a3b-9c65-ca9996bb0e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forêt à 10 arbres\n",
    "rf_10 = RandomForestClassifier(n_estimators = 10, random_state = 42)\n",
    "multi_rf_10 = MultiOutputClassifier(rf_10)\n",
    "# Permet de faire un modèle multi-label, i.e. de prédire plusieurs colonnes à la fois.\n",
    "\n",
    "# Entrainement\n",
    "multi_rf_10.fit(X_train_svd, Y_train)\n",
    "\n",
    "# Prédiction\n",
    "Y_pred = multi_rf_10.predict(X_test_svd)\n",
    "\n",
    "# 135 secondes pour n_estimators = 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb6a756-0c4e-4926-a86f-6f958f7368d5",
   "metadata": {},
   "source": [
    "Recherchons maintenant les meilleurs valeurs pour les hyperparamètres suivant :\n",
    "- `n_estimators` : nombre d'arbres.\n",
    "- `min_samples_split` : nombre minimal d'échantillons pour diviser un noeud.\n",
    "- `min_samples_leaf` : nombre minimal d'échantillons par feuille.\n",
    "- `max_depth` : profondeur maximale des arbres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a7cf24-f550-4db7-89f0-cb8bfb9157cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Une première recherche a permis de trouver la valeur optimale pour les 3 premiers paramètre en 2264 secondes.\n",
    "# On les fixe donc aux valeurs de sortie (10, 6, 1 resp) et on se concentre ici sur max_depth.\n",
    "rf = RandomForestClassifier(min_samples_leaf = 1, min_samples_split = 6, n_estimators = 10, random_state = 42)\n",
    "multi_rf = MultiOutputClassifier(rf)\n",
    "\n",
    "# Grille des hyperparamètres\n",
    "param_grid = {\n",
    "    # \"estimator__n_estimators\": [5, 10, 20],          # Nombre d'arbres\n",
    "    # \"estimator__min_samples_split\": [4, 6, 8],       # Nombre minimal d'échantillons pour diviser un noeud\n",
    "    # \"estimator__min_samples_leaf\": [1, 2, 3],        # Nombre minimal d'échantillons par feuille\n",
    "    \"estimator__max_depth\": [5, 10, 20, 50, 100]       # Profondeur maximale des arbres\n",
    "}\n",
    "# Les noms des hyperparamètres sont précédés de \"estimator__\" car le modèle est encapsulé dans 'multi_rf'.\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(multi_rf, param_grid, cv = 3, scoring = \"f1_micro\", verbose = 1, n_jobs = -1)\n",
    "# cv : divise le dataset en i et entraîne/teste sur ces différentes combinaisons.\n",
    "# scoring : métrique utilisée pour évaluer le modèle. Ici, \"f1_micro\" est utilisé car multi-label (f1 = métrique de base).\n",
    "# verbose : quantité de détails affichés lors de l'exécution.\n",
    "# n_jobs = -1 : tous les cv sont exécuté en parallèle pour accélérer la recherche.\n",
    "\n",
    "grid_search.fit(X_train_svd, Y_train)\n",
    "\n",
    "# Meilleurs paramètres\n",
    "print(\"Meilleurs paramètres :\", grid_search.best_params_)\n",
    "\n",
    "# Meilleure max_depth = 50 (trouvé en 230 secondes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9910826-36c6-499a-b77c-33cfc48d77e5",
   "metadata": {},
   "source": [
    "il aura donc fallu en tout environ 40 minutes pour trouver nos 4 hyperparamètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a77dcd3-a8b1-49be-8719-02678a9321c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions avec le meilleur modèle de random forest multi-label\n",
    "rf_opt = RandomForestClassifier(min_samples_leaf = 1, \n",
    "                                min_samples_split = 6, \n",
    "                                n_estimators = 10, \n",
    "                                max_depth = 50,\n",
    "                                random_state = 42)\n",
    "multi_rf_opt = MultiOutputClassifier(rf_opt)\n",
    "\n",
    "multi_rf_opt.fit(X_train_svd, Y_train)\n",
    "\n",
    "Y_pred = multi_rf_opt.predict(X_test_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54096db-919f-4c2f-b5a3-eadce83f605e",
   "metadata": {},
   "source": [
    "Il faut environ 4 minutes pour faire l'entraînement/prédiction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafcf4a2-8566-4ce7-9955-a2dc1d0b8094",
   "metadata": {},
   "source": [
    "Intéressons-nous enfin à l'analyse des résultats prédit par la Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff9474b-d264-45e0-b74d-9058c9df71f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rapport pour chaque catégorie : \\n\")\n",
    "for i, column in enumerate(Y_train.columns):\n",
    "    print(pd.DataFrame(confusion_matrix(Y_test.iloc[:, i], Y_pred[:, i]),\n",
    "             index = [\"0 donné\", \"1 donné\"],\n",
    "             columns = [\"0 prédit\", \"1 prédit\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e4ada-af19-4c6d-8107-d23590365ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser un DataFrame pour stocker les résultats\n",
    "results = []\n",
    "\n",
    "for i, column in enumerate(Y_train.columns):\n",
    "    # Calculer le rapport de classification\n",
    "    report = classification_report(Y_test.iloc[:, i], Y_pred[:, i], \n",
    "                          zero_division = 0,  # Évite les avertissements\n",
    "                          output_dict = True\n",
    "                         )\n",
    "    \n",
    "    # Calculer l'accuracy pour chaque étiquette\n",
    "    accuracy = accuracy_score(Y_test.iloc[:, i], Y_pred[:, i])\n",
    "    \n",
    "    # Ajouter les résultats au tableau\n",
    "    results.append({\n",
    "        \"Label\": column,\n",
    "        \"Precision\": report['weighted avg']['precision'],\n",
    "        \"Recall\": report['weighted avg']['recall'],\n",
    "        \"F1-Score\": report['weighted avg']['f1-score'],\n",
    "        \"Accuracy\": accuracy  # Ajouter l'accuracy à chaque étiquette\n",
    "    })\n",
    "\n",
    "# Afficher les résultats sous forme de tableau\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "print(\"\\n \\n Précision générale : \", accuracy_score(Y_test, Y_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b061540",
   "metadata": {},
   "source": [
    "Ce modèle permet de conduire à un taux d’erreur d’environ 10% concernant les prédictions, cela reste plus que raisonnable. Malgré tout, le temps d'exécution du modèle, en comptant la recherche d'optimisation d'hyper-paramètres, reste assez long, ce qui peut poser problème en terme de coût computationnnel suivant la machine. Un modèle plus rapide pourrait donc être à privilégier afin de réduire ce coût."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b461804a",
   "metadata": {},
   "source": [
    "# Méthodes des K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f39005e",
   "metadata": {},
   "source": [
    "La méthode des K-Nearest Neighbors est une méthode de classification non-supervisée et non-paramétrique, ce qui signfique qu'il n'est pas nécéssaire de faire des hypothèses concernant la distribution des données. Les méthodes non-paramétriques sont robustes et flexibles, ce qui induit qu'elles peuvent traiter des données biaisées, présentant des valeurs abbérantes ou ayant des échelles ou des unités différentes. Cette méthode se base sur le calcul des distances entre les différents individus. Lors de l'implémentation de la méthode des KNN, il est possible de choisir la distance calculée : distance euclidienne, distance de Ward, distance de Manhattan, etc. Cette distance peut être chosie par validation croisée. Dans notre cas, nous avons gardé la distance par defaut qui est la distance euclidienne car, après simulation, c'était celle avec le taux d'erreur le plus faible. \\\n",
    "Cette méthode consiste à regrouper les individus de manière itérative, en minimisant la disance avec leurs voisins selon k clusters. Il est également possible d'optimiser le paramètre k par validation croisée, ce qui a été fait dans ce cas présent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a40ffce",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "# Importation des packages\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "# Initialisation\n",
    "X_train = train[:, \"comment_clean\"]\n",
    "Y_train = train[:, \"toxic\"]\n",
    "X_test = test[:, \"comment_clean\"]\n",
    "Y_test = test[:, \"toxic\"]\n",
    "\n",
    "# Choix de la valeur k par validation croisée\n",
    "tx_erreur = []\n",
    "k = range(1, 100)\n",
    "\n",
    "for i in k:\n",
    "    models = KNeighborsClassifier(n_neighbors=i)\n",
    "    models.fit(X_train, Y_train)\n",
    "    pred_i = models.predict(X_test)\n",
    "    tx_erreur.append(np.mean(pred_i != Y_test))\n",
    "\n",
    "best_k = []\n",
    "for i in range (0, 99):\n",
    "    if tx_erreur[i+1] <= tx_erreur[i]\n",
    "    best_k = i+1\n",
    "\n",
    "print(best_k)\n",
    "\n",
    "# Algorithme de création du modèle K-NN\n",
    "model = KNeighborsClassifier(n_neighbors=best_k)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Prédictions des invidus test\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "# Calcul du taux d'erreur\n",
    "print(confusion_matrix(Y_test, prediction))\n",
    "print(classification_report(Y_test, predictions))\n",
    "\n",
    "# tx_erreur = mean((prediction - data[, \"toxic\"])^2)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(tx_erreur)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5433f83c-9c90-413f-9832-ad2a7f12e322",
   "metadata": {},
   "source": [
    "# ***Comparaison des modèles***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9553dbb-a7ec-4658-81f0-99df9775c697",
   "metadata": {},
   "source": [
    "## ***SVM vs Régression logistique***\n",
    "En raison des limitations matérielles, j’ai utilisé une sous-partie du dataset (15 000 observations pour l’entraînement et 10 000 pour les tests), contrairement à mes collègues qui ont travaillé sur l’ensemble des données. Cela rend mes résultats avec le SVM difficilement comparables, car le modèle n’a pas été exposé à l’intégralité des données, ce qui pourrait biaiser les métriques obtenues, comme des scores plus élevés sur cet échantillon restreint. Le SVM présente des avantages notables : il est particulièrement performant sur des datasets de taille modérée et des classes déséquilibrées, grâce à l’argument class_weight=\"balanced\", et il modélise des relations complexes via des noyaux comme RBF tout en résistant au surapprentissage. De plus, l’optimisation des hyperparamètres (par GridSearch) a permis d’améliorer ses performances. Cependant, son principal inconvénient réside dans sa scalabilité : son temps d’exécution devient prohibitif sur des datasets volumineux ou très dimensionnels. Pour remédier à cela, il est souvent nécessaire de réduire à la fois le nombre de points (échantillonnage) et les dimensions (via PCA), car réduire uniquement les dimensions ne suffit pas à diminuer significativement le temps d’exécution. En comparaison, la régression logistique est plus rapide, même avec de grands volumes de données, grâce à sa simplicité basée sur des calculs matriciels, ce qui la rend plus adaptée aux très grands datasets tout en offrant des performances compétitives. En conclusion, bien que le SVM soit un excellent choix pour des problèmes complexes ou des datasets modérés, une comparaison équitable avec d’autres modèles, comme la régression logistique, nécessiterait d’évaluer tous les modèles sur l’ensemble des données pour tirer des conclusions définitives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42f63b7-7651-4aa6-8bb5-c76850f0bd4e",
   "metadata": {},
   "source": [
    "## ***Régression logistique vs Random Forest***\n",
    "\n",
    "Au vu de nos résultats de prédiction, les 2 modèles sont aussi précis l'un que l'autre pour prédire les étiquettes des commentaires. Egalement, les deux méthodes peuvent permettre de regarder l'importance des variables avec le score de permutation pour la régression logistique et le score de Gini pour la forêt aléatoire.\\\n",
    "Toutefois, le temps d'exécution de la régression logistique étant bien inférieure à celui de la Random Forest, le premier est donc ici à privilégier.\n",
    "De plus, la régression logistique permet d'effectuer des tests a postériori pour vérifier la significativité et d'obtenir plus d'indicateurs tels que les odds-ratio. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0082f18b-9694-4871-b70b-1270a9d6c3df",
   "metadata": {},
   "source": [
    "## ***Conclusion***\n",
    "Des trois modèles étudiés précédemment, la régression logistique donc semble le mieux adapté pour nos données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a911521-e079-4e4c-95fc-7e600fe4fda3",
   "metadata": {},
   "source": [
    "# ***Modèle implémenter en CLI***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ce94cb-6113-4a0a-920d-8de719ea343b",
   "metadata": {},
   "source": [
    "## Use your model\n",
    "\n",
    "**Use the best model to build a Command-Line Interface (*CLI*) that is launched by the command `./cli.py [options]` using the `argsparse` module, and that accepts in stdin (standard input) english sentences and classifies them, displaying the result and interesting metrics if relevant.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd601a3-00c9-409b-b288-dbd4dbc8d418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "import argparse\n",
    "import sys"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
